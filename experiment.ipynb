{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import the necessary libraries, including os, PIL, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\rawframes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_file_path):\n",
    "    # open and load the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_images(folder_path, num_images=None):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "                if (num_images) and (len(image_files) == num_images):\n",
    "                    break\n",
    "    return image_files\n",
    "\n",
    "def display_images(image_files):\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        img = Image.open(image_file)\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(img )\n",
    "        plt.title(os.path.basename(image_file))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def load_and_display_images(folder_path,  num_images=None):\n",
    "    \"\"\"\n",
    "    Load and display images from the specified folder.\n",
    "\n",
    "    :param folder_path: Path to the folder containing images.\n",
    "    :param num_images: Number of images to display.\n",
    "    \"\"\"\n",
    "    image_files = load_images(folder_path, num_images)\n",
    "    display_images(image_files)\n",
    "\n",
    "\n",
    "def load_and_display_images_with_bb(training_json_dict, video_id, num_images=None, save_folder=None, display_images=True):\n",
    "    training_keys = list(training_json_dict.keys())\n",
    "    dict_image_data = training_json_dict[training_keys[2]]\n",
    "    dict_annotation_data = training_json_dict[training_keys[3]]\n",
    "\n",
    "    images_df = pd.DataFrame(dict_image_data)\n",
    "    images_df = images_df[images_df['video_id'] == video_id]\n",
    "\n",
    "    annotation_df = pd.DataFrame(dict_annotation_data)\n",
    "\n",
    "    images_list = list(images_df.file_name)\n",
    "    if not num_images:\n",
    "        num_images = len(images_list)\n",
    "\n",
    "    image_arrays = []\n",
    "    for i, image_name in tqdm(enumerate(images_list[:num_images])):\n",
    "        image_id = int(images_df[images_df['file_name'] == image_name].id)\n",
    "        bbox = list(annotation_df[annotation_df['image_id'] == image_id]['bbox'])[0]\n",
    "        image_array = np.array(Image.open(os.path.join(dataset_path, image_name)))\n",
    "        image_arrays.append(image_array)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image_array)\n",
    "        plt.gca().add_patch(plt.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], edgecolor='green', facecolor='none', linewidth=2))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_folder:\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "            plt.savefig(os.path.join(save_folder, os.path.basename(image_name)))\n",
    "        \n",
    "        if display_images:\n",
    "            plt.show()\n",
    "\n",
    "def buv_json_to_csv(buv_json_file, output_csv_file, two_points_format=True, only_train_frames=False):\t\n",
    "    json_dict = load_json(buv_json_file)\n",
    "    dict_keys = list(json_dict.keys())\n",
    "    dict_image_data = json_dict[dict_keys[2]]\n",
    "    dict_annotation_data = json_dict[dict_keys[3]]\n",
    "\n",
    "    images_df = pd.DataFrame(dict_image_data)\n",
    "    if only_train_frames:\n",
    "        images_df = images_df[images_df['is_vid_train_frame'] == True]\n",
    "\n",
    "    annotation_df = pd.DataFrame(dict_annotation_data)\n",
    "\n",
    "\n",
    "    images_list = list(images_df.file_name)\n",
    "    print('image_list_size: ', len(images_list))\n",
    "\n",
    "    images_to_ignore_list = ['benign/x66ef02e7f1b9a0ef', 'benign/x63c9ba1377f35bf6', 'benign/x5a1c46ec6377e946']\n",
    "\n",
    "    with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['img_file', 'x1', 'y1', 'x2', 'y2', 'class_name']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for image_name in tqdm(images_list):\n",
    "\n",
    "            ignore = False\n",
    "            for image_to_ignore in images_to_ignore_list:\n",
    "                if image_to_ignore in image_name:\n",
    "                    ignore = True\n",
    "            if ignore:\n",
    "                print(f'Image {image_name} is ignored')\n",
    "                continue\n",
    "\n",
    "            image_id = int(images_df[images_df['file_name'] == image_name].id)\n",
    "            bbox = list(annotation_df[annotation_df['image_id'] == image_id]['bbox'])[0]\n",
    "\n",
    "            if two_points_format:\n",
    "                writer.writerow({\n",
    "                    'img_file': os.path.join(dataset_path, image_name),\n",
    "                    'x1': bbox[0] if bbox[2]>=0 else bbox[0] + bbox[2],\n",
    "                    'y1': bbox[1] if bbox[3]>=0 else bbox[1] + bbox[3],\n",
    "                    'x2': bbox[0] + bbox[2] if bbox[2]>=0 else bbox[0],\n",
    "                    'y2': bbox[1] + bbox[3] if bbox[3]>=0 else bbox[1],\n",
    "                    'class_name': 'Lesion'\n",
    "                })\n",
    "            else:\n",
    "                writer.writerow({\n",
    "                            'img_file': os.path.join(dataset_path, image_name),\n",
    "                            'x1': bbox[0],\n",
    "                            'y1': bbox[1],\n",
    "                            'x2': bbox[2],\n",
    "                            'y2': bbox[3],\n",
    "                            'class_name': 'Lesion'\n",
    "                        })\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_json_file = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\imagenet_vid_train_15frames.json'\n",
    "training_csv_file = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\train_annotations_2.csv'\n",
    "\n",
    "#buv_json_to_csv(training_json_file, training_csv_file, only_train_frames=True)\n",
    "\n",
    "validation_json_file = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\imagenet_vid_val.json'\n",
    "validation_csv_file = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\val_annotations.csv'\n",
    "\n",
    "#buv_json_to_csv(validation_json_file, validation_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_json_dict = load_json(r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\imagenet_vid_train_15frames.json')\n",
    "video_id = 63\n",
    "save_path = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\example_images\\63'\n",
    "load_and_display_images_with_bb(training_json_dict, video_id, save_folder=save_path, display_images=False)\n",
    "\n",
    "folder_path = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\rawframes\\benign\\2cda21c3aab26332'\n",
    "#load_and_display_images(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with the .json data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\rawframes\\benign'\n",
    "video_name = os.listdir(folder_path)\n",
    "file_name = os.listdir(os.path.join(folder_path,video_name[0]))\n",
    "\n",
    "training_json_dict = load_json(r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\imagenet_vid_train_15frames.json')\n",
    "training_keys = list(training_json_dict.keys())\n",
    "print(training_keys)\n",
    "\n",
    "video_index = 0\n",
    "\n",
    "video_name = training_json_dict[training_keys[1]][video_index]['name']\n",
    "video_folder_path = os.listdir(os.path.join(dataset_path,video_name))\n",
    "\n",
    "# print the loaded data\n",
    "print(list(training_json_dict.keys()))\n",
    "print(training_json_dict[list(training_json_dict.keys())[0]][0])\n",
    "print(training_json_dict[list(training_json_dict.keys())[1]][0])\n",
    "print(training_json_dict[list(training_json_dict.keys())[2]][29])\n",
    "print(training_json_dict[list(training_json_dict.keys())[3]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retinanet Evaluation on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_directory = r\"C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\pytorch-retinanet\\trained_networks\"\n",
    "sys.path.append('./pytorch-retinanet')\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, UnNormalizer, Normalizer\n",
    "from retinanet import csv_eval, coco_eval\n",
    "\n",
    "csv_val = r\"C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\val_annotations.csv\"\n",
    "csv_classes = r\"C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\class_name.csv\"\n",
    "dataset_val = CocoDataset('../Miccai 2022 BUV Dataset', set_name='imagenet_vid_val', transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "dataloader_val = DataLoader(dataset_val, num_workers=1, collate_fn=collater, batch_sampler=sampler_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_directory = os.path.join(models_directory, 'experiment_1')\n",
    "experiment_2_directory = os.path.join(models_directory, 'experiment_2')\n",
    "experiment_3_directory = os.path.join(models_directory, 'experiment_3')\n",
    "experiment_4_directory = os.path.join(models_directory, 'experiment_4')\n",
    "experiment_5_directory = os.path.join(models_directory, 'experiment_5')\n",
    "\n",
    "\n",
    "\n",
    "def validate_and_save(experiment_directory, coco_metrics=True, plot_froc=False, validation_json_file=None):\n",
    "    for model_name in os.listdir(experiment_directory):\n",
    "\n",
    "        if not model_name.endswith('.pt'):\n",
    "            continue\n",
    "\n",
    "        print(model_name)\n",
    "        if coco_metrics:\n",
    "            output_json_file = os.path.join(experiment_directory, f'{model_name}_coco_metrics.json')\n",
    "        else:\n",
    "            output_json_file = os.path.join(experiment_directory, f'{model_name}_metrics.json')\n",
    "\n",
    "        model_path = os.path.join(experiment_directory, model_name)\n",
    "        retinanet = torch.load(model_path)\n",
    "        if coco_metrics:\n",
    "            if not validation_json_file:\n",
    "                raise ValueError('validation_json_file must be provided when using COCO metrics')\n",
    "            #metrics_list = csv_eval.evaluate_coco_metrics(dataset_val, retinanet, buv_json_file=validation_json_file, model_path=model_path),\n",
    "            metrics_list = coco_eval.evaluate_coco(dataset_val, retinanet, model_path=model_path),\n",
    "        else:\n",
    "            metrics_list = csv_eval.evaluate(dataset_val,retinanet,iou_threshold=0.5,score_threshold=0.05,max_detections=1,save_path=None)\n",
    "        \n",
    "        if not os.path.exists(output_json_file):\n",
    "            with open(output_json_file, 'w') as f:\n",
    "                json.dump(metrics_list, f, indent=4)\n",
    "\n",
    "        if plot_froc:\n",
    "            thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "            csv_eval.evaluate_froc(dataset_val, retinanet, buv_json_file=validation_json_file, model_path=model_path, score_thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_json_file = r'C:\\Users\\zaid-\\OneDrive\\Desktop\\PMSD\\Implementation\\Miccai 2022 BUV Dataset\\imagenet_vid_val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_and_save(models_directory, validation_json_file=validation_json_file)\n",
    "#validate_and_save(experiment_2_directory, validation_json_file=validation_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = 'C:/Users/zaid-/OneDrive/Desktop/PMSD/Implementation/pytorch-retinanet/trained_networks/experiment_4/coco_retinanet_13'\n",
    "model = model_path + '.pt'\n",
    "model = torch.load(model)\n",
    "torch.save(model.state_dict(), f'{model_path}_model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMSD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
